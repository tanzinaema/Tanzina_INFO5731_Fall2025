{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanzinaema/Tanzina_INFO5731_Fall2025/blob/main/For%20Practice/INFO5731_Assignment_2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Monday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install semanticscholar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKxGAPPkED3l",
        "outputId": "34c14a96-a769-4269-cc5b-853dd4156183"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: semanticscholar in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.12/dist-packages (from semanticscholar) (8.5.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from semanticscholar) (0.28.1)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from semanticscholar) (1.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->semanticscholar) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx->semanticscholar) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->semanticscholar) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx->semanticscholar) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->semanticscholar) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx->semanticscholar) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx->semanticscholar) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "bdf0fcd0-b158-47c3-d2dc-aecc8c3657ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting papers for query: machine learning\n",
            "  ‚Üí Collected 1000 papers for query: machine learning\n",
            "\n",
            "Collecting papers for query: data science\n",
            "  ‚Üí Collected 1000 papers for query: data science\n",
            "\n",
            "Collecting papers for query: artificial intelligence\n",
            "  ‚Üí Collected 1000 papers for query: artificial intelligence\n",
            "\n",
            "Collecting papers for query: information extraction\n",
            "  ‚Üí Collected 1000 papers for query: information extraction\n",
            "\n",
            "‚úÖ Final total: 4000 papers saved to semantic_scholar_top10000.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "from semanticscholar import SemanticScholar\n",
        "import time\n",
        "\n",
        "# Ignore warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Initialize Semantic Scholar client\n",
        "sem_sch = SemanticScholar()\n",
        "\n",
        "# Queries\n",
        "queries = [\"machine learning\", \"data science\", \"artificial intelligence\", \"information extraction\"]\n",
        "\n",
        "# Collect all results in a list first\n",
        "papers = []\n",
        "\n",
        "for query in queries:\n",
        "    print(f\"\\nCollecting papers for query: {query}\")\n",
        "    count = 0\n",
        "\n",
        "    try:\n",
        "        # Fetch in batches of 100\n",
        "        for paper in sem_sch.search_paper(\n",
        "            query,\n",
        "            limit=100,  # API max\n",
        "            fields=['paperId', 'title', 'abstract', 'year', 'citationCount']\n",
        "        ):\n",
        "            papers.append({\n",
        "                'query': query,\n",
        "                'paperId': paper.paperId,\n",
        "                'title': paper.title,\n",
        "                'abstract': paper.abstract,\n",
        "                'year': paper.year,\n",
        "                'citations': paper.citationCount\n",
        "            })\n",
        "\n",
        "            count += 1\n",
        "            if count >= 2500:  # cap at 2500 per query\n",
        "                break\n",
        "\n",
        "        print(f\"  ‚Üí Collected {count} papers for query: {query}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "\n",
        "# Convert list ‚Üí DataFrame once\n",
        "df_paper = pd.DataFrame(papers)\n",
        "\n",
        "# Save to CSV\n",
        "df_paper.to_csv(\"semantic_scholar_top10000.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"\\n‚úÖ Final total: {len(df_paper)} papers saved to semantic_scholar_top10000.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Define the API endpoint\n",
        "API_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "\n",
        "# Define the search queries and parameters\n",
        "queries = [\"machine learning\", \"data science\", \"artificial intelligence\", \"information extraction\"]\n",
        "papers_per_query = 2500  # 2500 √ó 4 = 10,000 total\n",
        "batch_size = 100         # max allowed by API per request\n",
        "\n",
        "# Store all paper results here\n",
        "all_papers = []\n",
        "\n",
        "for query in queries:\n",
        "    print(f\"\\nüîç Collecting top {papers_per_query} papers for query: '{query}'...\")\n",
        "    offset = 0\n",
        "\n",
        "    while offset < papers_per_query:\n",
        "        params = {\n",
        "            \"query\": query,\n",
        "            \"offset\": offset,\n",
        "            \"limit\": batch_size,\n",
        "            \"sort\": \"citationCount\",  # highest cited papers first\n",
        "            \"fields\": \"title,abstract,year,authors,citationCount\"\n",
        "        }\n",
        "\n",
        "        response = requests.get(API_URL, params=params)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"‚ùå Error at offset {offset}: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        data = response.json().get(\"data\", [])\n",
        "        if not data:\n",
        "            print(\"‚ö†Ô∏è No more results.\")\n",
        "            break\n",
        "\n",
        "        for paper in data:\n",
        "            all_papers.append({\n",
        "                \"query\": query,\n",
        "                \"title\": paper.get(\"title\"),\n",
        "                \"abstract\": paper.get(\"abstract\"),\n",
        "                \"year\": paper.get(\"year\"),\n",
        "                \"citations\": paper.get(\"citationCount\"),\n",
        "                \"authors\": \", \".join([author[\"name\"] for author in paper.get(\"authors\", [])])\n",
        "            })\n",
        "\n",
        "        offset += batch_size\n",
        "        time.sleep(0.2)  # to avoid rate limiting\n",
        "\n",
        "    print(f\"‚úÖ Collected {offset} papers for '{query}'\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(all_papers)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(\"semantic_scholar_top10000.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "# Print first 5 rows\n",
        "print(\"\\nüìÑ First 5 papers collected:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbbLr34fJezG",
        "outputId": "7d062e62-524f-4ee3-d19b-5f278a519d18"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Collecting top 2500 papers for query: 'machine learning'...\n",
            "‚ùå Error at offset 900: 429\n",
            "‚úÖ Collected 900 papers for 'machine learning'\n",
            "\n",
            "üîç Collecting top 2500 papers for query: 'data science'...\n",
            "‚ùå Error at offset 200: 429\n",
            "‚úÖ Collected 200 papers for 'data science'\n",
            "\n",
            "üîç Collecting top 2500 papers for query: 'artificial intelligence'...\n",
            "‚ùå Error at offset 200: 429\n",
            "‚úÖ Collected 200 papers for 'artificial intelligence'\n",
            "\n",
            "üîç Collecting top 2500 papers for query: 'information extraction'...\n",
            "‚ùå Error at offset 0: 429\n",
            "‚úÖ Collected 0 papers for 'information extraction'\n",
            "\n",
            "üìÑ First 5 papers collected:\n",
            "              query                                              title  \\\n",
            "0  machine learning  Fashion-MNIST: a Novel Image Dataset for Bench...   \n",
            "1  machine learning                  Physics-informed machine learning   \n",
            "2  machine learning  TensorFlow: Large-Scale Machine Learning on He...   \n",
            "3  machine learning  A Survey on Bias and Fairness in Machine Learning   \n",
            "4  machine learning  Stop explaining black box machine learning mod...   \n",
            "\n",
            "                                            abstract    year  citations  \\\n",
            "0  We present Fashion-MNIST, a new dataset compri...  2017.0       9311   \n",
            "1                                               None  2021.0       4337   \n",
            "2  TensorFlow is an interface for expressing mach...  2016.0      11258   \n",
            "3  With the widespread use of artificial intellig...  2019.0       4718   \n",
            "4                                               None  2018.0       6674   \n",
            "\n",
            "                                             authors  \n",
            "0            Han Xiao, Kashif Rasul, Roland Vollgraf  \n",
            "1  G. Karniadakis, I. Kevrekidis, Lu Lu, P. Perdi...  \n",
            "2  Mart√≠n Abadi, Ashish Agarwal, P. Barham, E. Br...  \n",
            "3  Ninareh Mehrabi, Fred Morstatter, N. Saxena, K...  \n",
            "4                                           C. Rudin  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "API_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "queries = [\"machine learning\", \"data science\", \"artificial intelligence\", \"information extraction\"]\n",
        "papers_per_query = 2500\n",
        "batch_size = 100\n",
        "\n",
        "all_papers = []\n",
        "\n",
        "for query in queries:\n",
        "    print(f\"\\nüîç Collecting top {papers_per_query} papers for query: '{query}'...\")\n",
        "    offset = 0\n",
        "\n",
        "    while offset < papers_per_query:\n",
        "        params = {\n",
        "            \"query\": query,\n",
        "            \"offset\": offset,\n",
        "            \"limit\": batch_size,\n",
        "            \"sort\": \"citationCount\",\n",
        "            \"fields\": \"title,abstract,year,authors,citationCount\"\n",
        "        }\n",
        "\n",
        "        response = requests.get(API_URL, params=params)\n",
        "\n",
        "        # Handle rate limiting (HTTP 429)\n",
        "        if response.status_code == 429:\n",
        "            print(f\"‚è≥ Hit rate limit at offset {offset}. Sleeping for 60 seconds...\")\n",
        "            time.sleep(60)\n",
        "            continue  # retry same offset\n",
        "\n",
        "        # Handle other request errors\n",
        "        if response.status_code != 200:\n",
        "            print(f\"‚ùå Error at offset {offset}: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        data = response.json().get(\"data\", [])\n",
        "        if not data:\n",
        "            print(\"‚ö†Ô∏è No more results.\")\n",
        "            break\n",
        "\n",
        "        for paper in data:\n",
        "            all_papers.append({\n",
        "                \"query\": query,\n",
        "                \"title\": paper.get(\"title\"),\n",
        "                \"abstract\": paper.get(\"abstract\"),\n",
        "                \"year\": paper.get(\"year\"),\n",
        "                \"citations\": paper.get(\"citationCount\"),\n",
        "                \"authors\": \", \".join([author[\"name\"] for author in paper.get(\"authors\", [])])\n",
        "            })\n",
        "\n",
        "        offset += batch_size\n",
        "        time.sleep(0.2)  # be polite\n",
        "\n",
        "    print(f\"‚úÖ Collected {offset} papers for '{query}'\")\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(all_papers)\n",
        "df.to_csv(\"semantic_scholar_top10000.csv\", index=False, encoding=\"utf-8\")\n",
        "\n",
        "# Print sample\n",
        "print(\"\\nüìÑ First 5 papers collected:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6waXYjdGKSRA",
        "outputId": "fd5818c7-ba09-4f7a-c1b7-6f778b28cc3a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Collecting top 2500 papers for query: 'machine learning'...\n",
            "‚è≥ Hit rate limit at offset 300. Sleeping for 60 seconds...\n",
            "‚è≥ Hit rate limit at offset 400. Sleeping for 60 seconds...\n",
            "‚è≥ Hit rate limit at offset 600. Sleeping for 60 seconds...\n",
            "‚è≥ Hit rate limit at offset 600. Sleeping for 60 seconds...\n",
            "‚ùå Error at offset 1000: 400\n",
            "‚úÖ Collected 1000 papers for 'machine learning'\n",
            "\n",
            "üîç Collecting top 2500 papers for query: 'data science'...\n",
            "‚è≥ Hit rate limit at offset 800. Sleeping for 60 seconds...\n",
            "‚ùå Error at offset 1000: 400\n",
            "‚úÖ Collected 1000 papers for 'data science'\n",
            "\n",
            "üîç Collecting top 2500 papers for query: 'artificial intelligence'...\n",
            "‚è≥ Hit rate limit at offset 700. Sleeping for 60 seconds...\n",
            "‚ùå Error at offset 1000: 400\n",
            "‚úÖ Collected 1000 papers for 'artificial intelligence'\n",
            "\n",
            "üîç Collecting top 2500 papers for query: 'information extraction'...\n",
            "‚è≥ Hit rate limit at offset 800. Sleeping for 60 seconds...\n",
            "‚ùå Error at offset 1000: 400\n",
            "‚úÖ Collected 1000 papers for 'information extraction'\n",
            "\n",
            "üìÑ First 5 papers collected:\n",
            "              query                                              title  \\\n",
            "0  machine learning  Fashion-MNIST: a Novel Image Dataset for Bench...   \n",
            "1  machine learning                  Physics-informed machine learning   \n",
            "2  machine learning  TensorFlow: Large-Scale Machine Learning on He...   \n",
            "3  machine learning  A Survey on Bias and Fairness in Machine Learning   \n",
            "4  machine learning  Stop explaining black box machine learning mod...   \n",
            "\n",
            "                                            abstract    year  citations  \\\n",
            "0  We present Fashion-MNIST, a new dataset compri...  2017.0       9311   \n",
            "1                                               None  2021.0       4337   \n",
            "2  TensorFlow is an interface for expressing mach...  2016.0      11258   \n",
            "3  With the widespread use of artificial intellig...  2019.0       4718   \n",
            "4                                               None  2018.0       6674   \n",
            "\n",
            "                                             authors  \n",
            "0            Han Xiao, Kashif Rasul, Roland Vollgraf  \n",
            "1  G. Karniadakis, I. Kevrekidis, Lu Lu, P. Perdi...  \n",
            "2  Mart√≠n Abadi, Ashish Agarwal, P. Barham, E. Br...  \n",
            "3  Ninareh Mehrabi, Fred Morstatter, N. Saxena, K...  \n",
            "4                                           C. Rudin  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QX6bJjGWXY9"
      },
      "outputs": [],
      "source": [
        "# Write code for each of the sub parts with proper comments.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0oOSlsOS0cq"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ],
      "metadata": {
        "id": "EcVqy1yj3wja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4 (20 points)."
      ],
      "metadata": {
        "id": "kEdcyHX8VaDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub‚Äôs usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ],
      "metadata": {
        "id": "1Ung5_YW3C6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ],
      "metadata": {
        "id": "CTOfUpatronW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4dtco9K--ks6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ],
      "metadata": {
        "id": "3WeD70ty3Gui"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qYRO5Cn8bYwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write your response below\n",
        "Fill out survey and provide your valuable feedback.\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSd_ObuA3iNoL7Az_C-2NOfHodfKCfDzHZtGRfIker6WyZqTtA/viewform?usp=dialog"
      ],
      "metadata": {
        "id": "JbTa-jDS-KFI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}